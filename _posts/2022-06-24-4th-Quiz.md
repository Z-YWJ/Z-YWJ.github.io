---
layout: post
title: 4th-Quiz
tags: 生信学习
toc:  true
math: true 
---

# **Quiz - Precision Medicine**

## **Summary**

[Liquid biopsy](https://z-ywj.github.io/2022/06/18/2nd-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/) can achieve personalized medical treatment including early screening, prognosis, and medication guidance of cancer by accurately analyzing the tumor components in the plasma of the subject. Based on this principle, this report analyzes the expression and composition of long RNA in plasma of 5 cancer patients and healthy people, and constructs various machine learning models. Through these models, we can give some potential possible extracellular RNA(exRNA) biomarkers for cancer early detection.

## **Dataset and Methods**

1. **Dataset**

   The data analyzed in this report comes from the bam file of long RNA paired-end sequencing given on the P Cluster. The dataset has a total of 373 samples, including extracellular long-RNA sequencing data of colon cancer (CRC), gastric cancer (STAD), lung cancer (LUAD), esophagus(ESCA), liver cancer (HCC) and healthy people(NC).

2. **Construct count matrix**

   Since the given data had been preprocessed, I proceeded featureCount directly to the various BAM files and completed the construction of the counts matrix using the given script summarize-table.py

3. **Matrix processing**

   For the count matrix, I used R-package "edgeR" to filter the low-expressed genes, normalized the matrix to remove the influence of sequencing depth, and removed the batch effect using the R-package "ComBat". Next, I tried to explore the various feature scaling method. At each processing step, I did a certain amount of data visualization to analyze whether the processing achieved the expected results.

4. **Feature selection and model construction**

    Here, I respectively applied three feature selection methods to construct three models:

    1) filter: using R-packet "DESeq2", I choose top 40 differentially expressed genes of HCC and NC as input features. Then, I construct binary classification model: "decision Tree" and "SVM (kernel function is radial)". The former is built with R-package-rpart, and the latter is built with R-package-e1071.

    2) embedded: using R-package-glmnet, I adopt "Lasso-logistic-regression(Lasso-LR)" to implemente feature selection and output binary classification model during training.

    3) wrapper: using R-package-randomForest, I construct "randomforest" model. This model uses different feature subsets for training and outputs finally multi-classification results

5. **Model training**
   1) SVM

   In SVM model, I use 10x cross validation in discover data to tune parameters $epsilon$(松弛变量) and $cost$(惩罚因子), and then select parameters that let model performance best as output.

   ![svm_tune](../../../../Files/SVM-cv.png)

   Figure 1. visualization of tuning the model SVM
   
   2) Lasso-logistic regression

   In lasso-LR model, I use 6x cross validation in discover data to tune parameters $C$(正则化因子), and then select parameters that let model's AUC best as output.

   ![lasso_tune](../../../../Files/lasso-cv.png)

   Figure 2. visualization of tuning the model Lasso-LR

6. **Model evaluation**

    * For the binary classification results of the first and second models, the AUC value was evaluated using R-package-pROC, and the expression heatmap was output
    * For the multi-classification results of the third model, an integrated AUC value was performed using "multiclass.roc" function of R-package-pRO

## **Results**

### **Matrix processing: filter, normalization, remove batch effect, feature scaling**

**1. filter**
   The obtained counts matrix is 60605 genes x 373 samples. In order to avoid dimensional disaster, I use edgeR::filterByExpr to remove some low-expressed genes, and the processed matrix is 24137 genes and 373 samples.

   ![filter](../../../../Files/filter.png)

   Figure 3. Distribution of the number of samples of gene expression 

   ![top50](../../../../Files/top-50.png)

   Figure 4. the top 50 expression genes of all samples

**2. normalization**

   Since there may be differences in sequencing depth between different samples, we calculated the log2(CPM) and log2(TMM) values of the count matrix. By observing relative logarithmic expression(RLE) boxplot, I selected log2(TMM) matrix for subsequent processing.
   
   ![counts](../../../../Files/log_counts.png)
  
   Figure 5. RLE plot of log2(counts matrix)

   ![cpm](../../../../Files/log_CPM.png)

   Figure 6. RLE plot of log2(CPM) matrix

   ![tmm](../../../../Files/log_TMM.png)

   Figure 7.RLE plot of log2(TMM) matrix

**3. remove batch effect**
  Due to the difference in time and operation, even the experimental data generated from the same sample may have great differences. That is batch effect. Therefore, here we adopt the ComBat method to remove batch effects.

 ![batch](../../../../Files/batch.png)
  
  Figure 8. the batch effect was visualized before and after ComBat

 ![pca_counts](../../../../Files/pca_counts.png)
  
  Figure 9. PCA plot to view batch effect on log2(counts) matrix

 ![pca_tmm](../../../../Files/pca_tmm.png)
 
 Figure 10. PCA to view batch effect on log2(TMM) matrix

**4. feature scaling**

  Since the variation range between different features may be different, here means the expression abundance of different genes is different. This property will have a negative impact on the subsequent model training, such as large-scale features often play a decisive role, the features with small-scale may be ignored. Therefore, we need to perform feature scale.

  ![scale](../../../../Files/scaling.png)

Figure 11. visualize the effect of different scale methods on the data

### **model construction and evaluation**

After finshing the matrix processing, we can filter features and constuct model to classify cancer patients from the health population.

#### **Binary classification —— HCC vs NC**

 In this part, we filter top 40 differential expression gene(DEGs) between samples of HCC and NC, and taking them as features input model "Decision Tree" & "SVM". Finally, We obtained AUC values of 0.856 and 0.823 on the validation data respectively.

- filter features by differential expression

![heatmap](../../../../Files/diff-gene.png)

Figure 12. expression of Top40 genes in discover data(left) and validation data(right)

- Decision tree
  
  ![tree](../../../../Files/tree.png)

Figure 13. the decision tree

 ![tree_AUC](../../../../Files/diff-decision-tree.png)

Figure 14. the AUC of decision tree model is 0.856

- SVM

![svm_AUC](../../../../Files/diff-SVM.png)

Figure 15. the AUC of SVM model is 0.823

#### **Binary classification —— Cancer vs health**

In this part, I use Lasso-Logistic Regession to implement feature selection and binary classify between cancer and health.

![lasso-filter](../../../../Files/lasso-visul.png)

Figure 16. visualization of Lasso-LR filtering feature

![lasso-LR](../../../../Files/lasso-auc.png)

Figure 17. the AUC of Lasso-LR model is 0.869

#### **Multiclass classification**

In this part, we using randomforset model to filter 40 features and multi-classify five groups of people. The intergrated AUC on the validation data is 0.8865.

> annotation: because number of samples of LUAD and ESCA is too less, so I view them as a class.

![heatmap](../../../../Files/rf-feature.png)

Figure 18. expression heatmap of 40 features filtered by randomforest

![multi_ROC](../../../../Files/ROC_rf_test.png)

Figure 19. multi ROC curve(may be not very accurate)

